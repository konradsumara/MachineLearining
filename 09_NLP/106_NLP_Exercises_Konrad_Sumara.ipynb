{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercises\n",
    "\n",
    "We have five exercises in this section. The exercises are:\n",
    "1. Build your own tokenizer, where you need to implement two functions to implement a tokenizer based on regular expression.\n",
    "2. Get tags from Trump speech.\n",
    "3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "4. Build your own Bag Of Words implementation using tokenizer created before.\n",
    "5. Build a 5-gram model and clean up the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Build your own tokenizer\n",
    "\n",
    "Build two different tokenizers:\n",
    "- ``tokenize_sentence``: function tokenizing text into sentences,\n",
    "- ``tokenize_word``: function tokenizing text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing necessary libraries\n",
    "\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions of new functions\n",
    "\n",
    "def tokenize_words(text: str) -> list:\n",
    "    \"\"\"Tokenize text into words using regex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "            Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "            List containing words tokenized from text\n",
    "\n",
    "    \"\"\"\n",
    "    list_of_words = re.split(r'\\W+', text)   \n",
    "    return list_of_words\n",
    "\n",
    "def tokenize_sentence(text: str) -> list:\n",
    "    \"\"\"Tokenize text into words using regex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "            Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "            List containing words tokenized from text\n",
    "\n",
    "    \"\"\"\n",
    "    list_of_sentences =  re.split('(?<=[.!?])',text)   \n",
    "    return list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Tokenized sentences:\n",
      "\n",
      "['Here we go again.', ' I was supposed to add this text later.', \"Well, it's 10.\", 'p.', 'm.', \" here, and I'm actually having fun making this course.\", ' :oI hope you are getting along fine with this presentation, I really did try.', 'And one last sentence, just so you can test you tokenizers better.', '']\n",
      "\n",
      "--> Tokenized words:\n",
      "\n",
      "['Here', 'we', 'go', 'again', 'I', 'was', 'supposed', 'to', 'add', 'this', 'text', 'later', 'Well', 'it', 's', '10', 'p', 'm', 'here', 'and', 'I', 'm', 'actually', 'having', 'fun', 'making', 'this', 'course', 'oI', 'hope', 'you', 'are', 'getting', 'along', 'fine', 'with', 'this', 'presentation', 'I', 'really', 'did', 'try', 'And', 'one', 'last', 'sentence', 'just', 'so', 'you', 'can', 'test', 'you', 'tokenizers', 'better', '']\n"
     ]
    }
   ],
   "source": [
    "# Checking function performance\n",
    "\n",
    "text = \"Here we go again. I was supposed to add this text later.\\\n",
    "Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\\\n",
    "I hope you are getting along fine with this presentation, I really did try.\\\n",
    "And one last sentence, just so you can test you tokenizers better.\"\n",
    "\n",
    "print(\"--> Tokenized sentences:\\n\")\n",
    "print(tokenize_sentence(text))\n",
    "\n",
    "print(\"\\n--> Tokenized words:\\n\")\n",
    "print(tokenize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Get tags from Trump speech using NLTK\n",
    "\n",
    "You should use the ``trump.txt`` file, read it and find the tags for each word. Use NLTK for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import nltk \n",
    "from nltk import word_tokenize  \n",
    "from nltk import pos_tag     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file\n",
    "\n",
    "file = open(\"../datasets/trump.txt\", \"r\",encoding=\"utf-8\") \n",
    "trump = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Tagged words from Trump's speach:\n",
      "\n",
      "[('Thank', 'NNP'), ('you', 'PRP'), ('very', 'RB'), ('much', 'RB'), ('.', '.'), ('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Mr.', 'NNP'), ('Vice', 'NNP'), ('President', 'NNP'), (',', ','), ('Members', 'NNP'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('the', 'DT'), ('First', 'NNP'), ('Lady', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('and', 'CC'), ('citizens', 'NNS'), ('of', 'IN'), ('America', 'NNP'), (':', ':'), ('Tonight', 'NN'), (',', ','), ('as', 'IN'), ('we', 'PRP'), ('mark', 'VBP'), ('the', 'DT'), ('conclusion', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('celebration', 'NN'), ('of', 'IN'), ('Black', 'NNP'), ('History', 'NNP'), ('Month', 'NNP'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('reminded', 'VBN'), ('of', 'IN'), ('our', 'PRP$'), ('Nation', 'NN'), (\"'s\", 'POS'), ('path', 'NN'), ('towards', 'NNS'), ('civil', 'JJ'), ('rights', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('work', 'NN'), ('that', 'WDT'), ('still', 'RB'), ('remains', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('done', 'VBN'), ('.', '.'), ('Recent', 'JJ'), ('threats', 'NNS'), ('targeting', 'VBG'), ('Jewish', 'NNP'), ('community', 'NN'), ('centers', 'NNS'), ('and', 'CC'), ('vandalism', 'NN'), ('of', 'IN'), ('Jewish', 'JJ'), ('cemeteries', 'NNS'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('last', 'JJ'), ('week', 'NN'), (\"'s\", 'POS'), ('shooting', 'NN'), ('in', 'IN'), ('Kansas', 'NNP'), ('City', 'NNP'), (',', ','), ('remind', 'VBP'), ('us', 'PRP'), ('that', 'IN'), ('while', 'IN'), ('we', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('a', 'DT'), ('nation', 'NN'), ('divided', 'VBN'), ('on', 'IN'), ('policies', 'NNS')] \n",
      "\n",
      "--> For the purpose of not littering the display with excess amount of output, I'm only printing here the first 100 out of 5685 words with their tags (the list_of_words_tagged list contains all words with their tags).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and tagging words in file\n",
    "\n",
    "list_of_words = word_tokenize(trump)\n",
    "list_of_words_tagged = pos_tag(list_of_words)\n",
    "\n",
    "N_words_printed = 100\n",
    "N_words_total = len(word_tokenize(trump))\n",
    "\n",
    "print(\"--> Tagged words from Trump's speach:\\n\")\n",
    "\n",
    "print(list_of_words_tagged[:N_words_printed],\"\\n\")\n",
    "\n",
    "print(\"--> For the purpose of not littering the display with excess amount of output, \\\n",
    "I'm only printing here the first\",N_words_printed,\"out of\",N_words_total,\"words with their tags (the list_of_words_tagged list contains all words with their tags).\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "\n",
    "Please use Python list features to get the last 10 sentences and display nouns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing necessary libraries\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize # Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading file\n",
    "\n",
    "file = open(\"../datasets/trump.txt\", \"r\",encoding='utf-8') \n",
    "trump = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Last 10 sentences in Trump's speach are:\n",
      "\n",
      "When we fulfill this vision, when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American greatness began.The time for small thinking is over.The time for trivial fights is behind us.We just need the courage to share the dreams that fill our hearts, the bravery to express the hopes that stir our souls, and the confidence to turn those hopes and those dreams into action.From now on, America will be empowered by our aspirations, not burdened by our fears; inspired by the future, not bound by failures of the past; and guided by our vision, not blinded by our doubts.I am asking all citizens to embrace this renewal of the American spirit.I am asking all Members of Congress to join me in dreaming big and bold, and daring things for our country.I am asking everyone watching tonight to seize this moment.Believe in yourselves, believe in your future, and believe, once more, in America.Thank you, God bless you, and God bless the United States. \n",
      "\n",
      "--> Nouns in last 10 sentences in Trump's speach are:\n",
      "\n",
      "vision  years  freedom  tonight  chapter  greatness  time  thinking  time  fights  courage  dreams  hearts  bravery  hopes  souls  confidence  hopes  dreams  action  aspirations  fears  future  failures  past  vision  doubts  citizens  renewal  things  country  tonight  moment  yourselves  future  \n",
      "\n",
      "--> Average number of nouns per sentence in last 10 sentences in Trump's speach is: 3.5\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text into sentences and selecting 10 last sentences\n",
    "\n",
    "N_sentences = 10\n",
    "\n",
    "trump_sentences = sent_tokenize(trump)\n",
    "trump_last_N_sentences = trump_sentences[len(trump_sentences) - N_sentences : ]\n",
    "trump = ''.join(trump_last_N_sentences)\n",
    "\n",
    "print(\"--> Last 10 sentences in Trump's speach are:\\n\")       \n",
    "print(trump,\"\\n\")\n",
    "\n",
    "nouns_count = 0\n",
    "\n",
    "print(\"--> Nouns in last 10 sentences in Trump's speach are:\\n\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "for token in nlp(trump):\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns_count = nouns_count + 1\n",
    "        print(token.text,\" \", end='')\n",
    "        \n",
    "print(\"\\n\\n--> Average number of nouns per sentence in last 10 sentences in Trump's speach is:\",nouns_count / N_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
    "- ``get_features_names`` - returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BagOfWords class\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation.\"\"\"\n",
    "    \n",
    "    __nlp = spacy.load(\"en_core_web_sm\")\n",
    "    __bow_list = []\n",
    "    __sentences = []\n",
    "    __words = []\n",
    "    __unique_words = []\n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        \"\"\"Transform list of strings into BoW array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: List[str]\n",
    "                Corpus of texts to be transforrmed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "                Matrix representation of BoW\n",
    "\n",
    "        \"\"\"\n",
    "        self.__sentences = corpus\n",
    "        \n",
    "        # Tokenizing corpus\n",
    "        for s in self.__sentences:          \n",
    "            for word in tokenize_words(s):\n",
    "                self.__words.append(word)\n",
    "                \n",
    "        # Removing empty string from the end of the list               \n",
    "        self.__words = list(set(self.__words))\n",
    "        self.__words.sort(key=lambda v: v.lower())   \n",
    "        if self.__words[0] == '':\n",
    "            self.__words.pop(0)\n",
    "        \n",
    "        # Selecting unique words\n",
    "        for i in self.__words:\n",
    "            if not i.lower() in self.__unique_words:\n",
    "                self.__unique_words.append(i.lower());\n",
    "              \n",
    "        # Calculating number of occurances of each word in the bag\n",
    "        for s in self.__sentences:\n",
    "            tmp = []\n",
    "            for word in self.__unique_words:\n",
    "                occurances = 0\n",
    "                for w in tokenize_words(s):\n",
    "                    if w.lower() == word:\n",
    "                        occurances = occurances+1\n",
    "                tmp.append(occurances)           \n",
    "            self.__bow_list.append(tmp)        \n",
    "        return np.array(self.__bow_list)   \n",
    "\n",
    "      \n",
    "\n",
    "    def get_feature_names(self) -> list:\n",
    "        \"\"\"Return words corresponding to columns of matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "                Words being transformed by fit function\n",
    "\n",
    "        \"\"\"   \n",
    "        # your code goes here\n",
    "        return self.__unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Bag Of Words displayed as DataFrame:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>as</th>\n",
       "      <th>bag</th>\n",
       "      <th>based</th>\n",
       "      <th>below</th>\n",
       "      <th>can</th>\n",
       "      <th>counting</th>\n",
       "      <th>document</th>\n",
       "      <th>documents</th>\n",
       "      <th>gives</th>\n",
       "      <th>...</th>\n",
       "      <th>really</th>\n",
       "      <th>see</th>\n",
       "      <th>sparse</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>throughout</th>\n",
       "      <th>us</th>\n",
       "      <th>words</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  as  bag  based  below  can  counting  document  documents  gives ...   \\\n",
       "0  0   0    1      1      0    0         1         0          0      0 ...    \n",
       "1  0   0    0      0      0    0         0         0          1      0 ...    \n",
       "2  0   0    0      0      0    0         0         1          0      0 ...    \n",
       "3  0   1    0      0      0    1         0         0          0      0 ...    \n",
       "4  1   0    0      0      2    0         0         0          0      1 ...    \n",
       "\n",
       "   really  see  sparse  the  third  this  throughout  us  words  you  \n",
       "0       0    0       0    0      0     0           0   0      1    0  \n",
       "1       0    0       0    0      0     0           1   0      1    0  \n",
       "2       0    0       0    1      1     1           0   0      0    0  \n",
       "3       0    1       0    1      0     0           0   0      1    1  \n",
       "4       1    2       1    0      0     1           0   1      0    0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Number of words (columns) in our Bag Of Words:  31 \n",
      "\n",
      "--> Number of documents(rows) in our Bag Of Words:  5 \n",
      "\n",
      "--> Words in a bag of words:  ['a', 'as', 'bag', 'based', 'below', 'can', 'counting', 'document', 'documents', 'gives', 'is', 'matrix', 'most', 'multiple', 'occur', 'occurences', 'of', 'on', 'once', 'only', 'pretty', 'really', 'see', 'sparse', 'the', 'third', 'this', 'throughout', 'us', 'words', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Checking how BagOfWords class works\n",
    "\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]    \n",
    "    \n",
    "vectorizer = BagOfWords()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#print(X)\n",
    "\n",
    "print(\"--> Bag Of Words displayed as DataFrame:\\n\")\n",
    "display(pd.DataFrame(X, columns= vectorizer.get_feature_names()))\n",
    "\n",
    "print(\"--> Number of words (columns) in our Bag Of Words: \", len(vectorizer.get_feature_names()),\"\\n\")\n",
    "print(\"--> Number of documents(rows) in our Bag Of Words: \", len(X),\"\\n\")\n",
    "\n",
    "print(\"--> Words in a bag of words: \", vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Build a 5-gram model and clean up the results.\n",
    "\n",
    "There are three tasks to do:\n",
    "1. Use 5-gram model instead of 3.\n",
    "2. Change to capital letter each first letter of a sentence.\n",
    "3. Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "Hint: for 2. and 3. implement a function called ``clean_generated()`` that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Loading necessary libraries\n",
    "\n",
    "from nltk.book import *\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of functions\n",
    "\n",
    "wall_street = text7.tokens\n",
    "tokens = wall_street\n",
    "\n",
    "# Cleanup function - deletes all meaningless words/characters\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()\n",
    "\n",
    "# build_ngrams - builds ngrams\n",
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams\n",
    "\n",
    "# ngram_freqs - calculates the frequency of tokens in each ngram and sum if there are more than one tokens related to a ngram\n",
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts\n",
    "\n",
    "# next_word - choose the next word by using the most recent tokens and adds it.\n",
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_generated - cleans generated text\n",
    "\n",
    "def clean_generated(generated):\n",
    "    generated = generated.replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\") \n",
    "    temp = [g for g in generated]\n",
    "    temp[0] = temp[0].upper()\n",
    "    \n",
    "    return \"\".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Generated text before cleaning and corrections:\n",
      "\n",
      " we have managed to maximize our direct-mail capability . In addition Buick is a relatively respected nameplate among American Express card holders says 0 an American Express spokeswoman . When the company asked members in a mailing which cars they would like to get information about for possible future purchases Buick came in fourth among U.S. cars and in the top 10 of all cars the spokeswoman says 0 . American Express has more than 24 million card holders in the U.S.\n",
      "\n",
      "--> Generated text after cleaning and corrections:\n",
      "\n",
      " We have managed to maximize our direct-mail capability. In addition Buick is a relatively respected nameplate among American Express card holders says 0 an American Express spokeswoman. When the company asked members in a mailing which cars they would like to get information about for possible future purchases Buick came in fourth among U.S. cars and in the top 10 of all cars the spokeswoman says 0. American Express has more than 24 million card holders in the U.S.\n"
     ]
    }
   ],
   "source": [
    "# Assessing performance of functions\n",
    "\n",
    "N=5 # fix it for other value of N\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq=\"We have managed to\" # Changed to first N-1 words\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "print(\"--> Generated text before cleaning and corrections:\\n\\n\",generated)\n",
    "\n",
    "generated = clean_generated(generated)\n",
    "\n",
    "print(\"\\n--> Generated text after cleaning and corrections:\\n\\n\",generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
